# -*- coding: utf-8 -*-
"""CP4  - Parte 1, 2 e 3- 1CCPF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11huo-lVoMr3d4-1s_LdSTPdYpGdO4irM

# **CP4 - Soluções em Energias Renovaveis - Parte 1, 2 e 3**

**Turma:** 1CCPF

**Integrantes:**
- MariaFernanda Garavelli Dantas RM: 562686
- Rogério Deligi Ferreira Filho RM: 561942
- Guilherme Morais de Assis  RM: 564198

# **Inicio da CP4**

Primeiramente baixando o arquivo household_power_comsumption.txt
"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/ColabData/household_power_consumption.txt'

"""# **Exercício 1**

Carregando o dataset e imprimindo as primeiras 10 linhas dele
"""

import pandas as pd

# Carregar o dataset (substitua o caminho pelo local onde você salvou o arquivo .txt ou .csv)
df = pd.read_csv(file_path, sep=";", na_values="?", low_memory=False)

# Exibir as 10 primeiras linhas
print(df.head(10))

"""# **Exercício 2**

Explicação da diferença entre Global_active_power e Global_reactive_power

***Diferença entre Global_active_power e Global_reactive_power***

  - *Global_active_power* (Potência Ativa)
É a potência efetivamente consumida/convertida em trabalho útil (W → kW).
Exemplo: ligar uma lâmpada, TV ou geladeira.
Essa energia é a que de fato aparece na conta de luz.

  - *Global_reactive_power* (Potência Reativa)
É a potência associada aos campos elétricos e magnéticos de dispositivos (motores, transformadores, etc.).
Não realiza trabalho útil, mas é necessária para manter os equipamentos funcionando.
É medida em kVAR (kilovolt-ampere reativo).

# **Exercício 3**

Verificando valores ausentes no dataset
"""

# Verificar valores ausentes
missing_values = df.isnull().sum()

# Quantidade total de valores ausentes
total_missing = df.isnull().sum().sum()

print("Valores ausentes por coluna:\n", missing_values)
print("\nTotal de valores ausentes:", total_missing)

"""# **Exercício 4**

Convertendo Date para datetime e criar coluna do dia da semana
"""

# Converter a coluna 'Date' para datetime
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

# Criar nova coluna com o dia da semana
df['Weekday'] = df['Date'].dt.day_name()

# Visualizar exemplo
print(df[['Date', 'Weekday']].head(10))

"""# **Exercício 5**

Filtrando registros de 2007 e calculando média diária do consumo (Global_active_power)
"""

# Filtrar apenas 2007
df_2007 = df[df['Date'].dt.year == 2007]

# Média diária
media_diaria_2007 = df_2007.groupby(df_2007['Date'])['Global_active_power'].mean()
print(media_diaria_2007.head())

"""# **Exercício 6**

Gerando gráfico de linha de um único dia
"""

import matplotlib.pyplot as plt

# Selecionar um dia (exemplo: 1º de março de 2007)
um_dia = df_2007[df_2007['Date'] == '2007-03-01']

plt.figure(figsize=(12,5))
plt.plot(um_dia['Time'], um_dia['Global_active_power'])
plt.title("Variação do Consumo em 01/03/2007")
plt.xlabel("Hora do dia")
plt.ylabel("Potência ativa (kW)")
plt.xticks(rotation=45)
plt.show()

"""# **Exercício 7**

Gerando histograma do Voltage
"""

plt.hist(df['Voltage'].dropna(), bins=50, color='skyblue', edgecolor='black')
plt.title("Distribuição da Tensão (Voltage)")
plt.xlabel("Voltage (V)")
plt.ylabel("Frequência")
plt.show()

"""### **Algumas Observações**

- A distribuição é aproximadamente normal (simétrica).
- O valor médio de tensão está em torno de 240 V.
- A maioria das medições ocorre entre 235 V e 245 V.
- Variações extremas (<230 V ou >250 V) são raras.

# **Exercício 8**

Calculando consumo médio por mês em todo o período
"""

df['YearMonth'] = df['Date'].dt.to_period('M')

consumo_mensal = df.groupby('YearMonth')['Global_active_power'].mean()
print(consumo_mensal)

"""# **Exercício 9**

Identificando o dia com maior consumo de energia ativa
"""

consumo_diario = df.groupby('Date')['Global_active_power'].sum()

dia_max = consumo_diario.idxmax()
print("Dia com maior consumo:", dia_max, "→", consumo_diario.max())

"""# **Exercício 10**

Comparando consumo médio: dias de semana vs finais de semana
"""

df['Weekday'] = df['Date'].dt.dayofweek  # 0=segunda, 6=domingo
df['is_weekend'] = df['Weekday'] >= 5

media_weekday = df.groupby('is_weekend')['Global_active_power'].mean()
print(media_weekday)

"""# **Exercício 11**

Calculando correlação entre variáveis
"""

corr = df[['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']].corr()
print(corr)

"""# **Exercício 12**

Criando nova variável Total_Sub_metering
"""

df['Total_Sub_metering'] = df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3']

"""# **Exercício 13**

Verificando se algum mês teve Total_Sub_metering maior que média de Global_active_power
"""

comparacao = df.groupby('YearMonth')[['Total_Sub_metering', 'Global_active_power']].mean()

resultado = comparacao[comparacao['Total_Sub_metering'] > comparacao['Global_active_power']]
print(resultado)

"""# **Exercício 14**

Gerando gráfio Voltage para o ano de 2008
"""

# Converta a coluna Time para timedelta
df['Time'] = pd.to_timedelta(df['Time'])

# Crie a coluna DateTime somando Date + Time
df['DateTime'] = df['Date'] + df['Time']

df_2008 = df[df['DateTime'].dt.year == 2008]

plt.figure(figsize=(12,5))
plt.plot(df_2008['DateTime'], df_2008['Voltage'], color='orange')
plt.title("Série Temporal do Voltage - 2008")
plt.xlabel("Data")
plt.ylabel("Voltage (V)")
plt.show()

"""# **Exercício 15**

Comparando consumo: verão x inverno (hemisfério norte)
"""

# Verão
verao = df[df['DateTime'].dt.month.isin([6,7,8])]
# Inverno
inverno = df[df['DateTime'].dt.month.isin([12,1,2])]

print("Consumo médio verão:", verao['Global_active_power'].mean())
print("Consumo médio inverno:", inverno['Global_active_power'].mean())

"""# **Exercício 16**

Aplicando amostragem e verificando distribuição da Global_active_power com a base completa
"""

sample = df.sample(frac=0.01, random_state=42)

plt.figure(figsize=(12,5))
df['Global_active_power'].hist(alpha=0.5, bins=50, label='Base Completa')
sample['Global_active_power'].hist(alpha=0.5, bins=50, label='Amostra 1%')
plt.legend()
plt.title("Comparação da distribuição de Global_active_power")
plt.show()

"""# **Exercício 17**

Utilizando min-max scaling para padronizar as variaveis numéricas
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_norm = df[['Global_active_power','Global_reactive_power','Voltage','Global_intensity']].copy()
df_norm[df_norm.columns] = scaler.fit_transform(df_norm)

df_norm.head()

"""# **Exercício 18**

Aplicando K-means para segmentar os 3 grupos distintos de consumo elétrico
"""

# Certifique-se de que a coluna 'Date' está no formato datetime
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

# Agrupar por dia e calcular a média de 'Global_active_power'
df_dia = df.groupby('Date')['Global_active_power'].mean().reset_index()

# Renomear a coluna para refletir que é uma média
df_dia.rename(columns={'Global_active_power': 'Mean_Global_active_power'}, inplace=True)

from sklearn.cluster import KMeans

df_dia['Mean_Global_active_power'] = df_dia['Mean_Global_active_power'].fillna(df_dia['Mean_Global_active_power'].mean())

kmeans = KMeans(n_clusters=3, random_state=42)
df_dia['Cluster'] = kmeans.fit_predict(df_dia[['Mean_Global_active_power']])

# Gráfico
plt.figure(figsize=(12,5))
plt.scatter(df_dia['Date'], df_dia['Mean_Global_active_power'], c=df_dia['Cluster'], cmap='viridis')
plt.title("Segmentação de Consumo Diário (KMeans)")
plt.xlabel("Data")
plt.ylabel("Média Global Active Power")
plt.show()

"""### **Interpretação do Gráfico:**

O gráfico mostra a segmentação do consumo diário ao longo do tempo (2007–2011) utilizando o algoritmo K-Means.

 -	Cada ponto representa o consumo diário em um determinado dia.

 -  As cores representam diferentes grupos (clusters) de consumo detectados pelo K-Means.

 -  Observa-se que existem três padrões principais:

 - Grupo amarelo → consumo baixo, mais estável.

 -  Grupo roxo → consumo médio, predominante na série.

 - Grupo verde → consumo mais alto, com picos sazonais que aparecem em certos períodos do ano.

Em resumo: o consumo apresenta ciclos sazonais, com momentos de maior intensidade (clusters verdes), mas a maior parte dos dias concentra-se em níveis médios (roxo) e baixos (amarelo).

# **Exercício 19**

Decompondo de serie temporal (sazonalidade, tendências e resíduo) para Global_active_power em um periodo de 6 meses
"""

from statsmodels.tsa.seasonal import seasonal_decompose

df_6m = df.set_index('DateTime').resample('D')['Global_active_power'].mean().dropna().head(180)

result = seasonal_decompose(df_6m, model='additive', period=30) # período mensal
result.plot()
plt.show()

"""# **Exercício 20**

Treinando modelo de regressão linear simples para prever Global_active_power a partir de Global_intensity
"""

#Preenchendo valores faltantes
df['Global_intensity'] = df['Global_intensity'].fillna(df['Global_intensity'].mean())
df['Global_active_power'] = df['Global_active_power'].fillna(df['Global_active_power'].mean())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

X = df[['Global_intensity']]
y = df['Global_active_power']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Erro quadrático médio (MSE):", mean_squared_error(y_test, y_pred))
print("Coeficiente:", model.coef_)
print("Intercepto:", model.intercept_)

"""### **Avaliando o erro:**

O modelo de regressão linear mostra uma relação direta entre intensidade elétrica e consumo de energia, com cada aumento de 1 Ampere resultando em cerca de 0,238 kW adicionais. O MSE baixo (0,00247) indica boa precisão nas previsões. O intercepto próximo de zero faz sentido fisicamente, pois sem corrente não há consumo. No geral, o modelo simples captura bem o comportamento observado.

# **CP4 - Parte 2**

# **Exercicío 21**

Series temporais por hora
"""

# -----------------------------
# Carregar e preparar os dados
# -----------------------------
df = pd.read_csv("household_power_consumption.txt", sep=";",
                parse_dates={"Datetime": ["Date", "Time"]},
                infer_datetime_format=True,
                na_values="?",
                low_memory=False)

# Converter para datetime e definir como índice
df['Datetime'] = pd.to_datetime(df['Datetime'])
df.set_index('Datetime', inplace=True)
# Converter Global_active_power para numérico
df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors="coerce")

# Reamostrar em intervalos de 1h (média)
hourly = df['Global_active_power'].resample('1H').mean()
# Média por hora do dia (0h até 23h)
mean_by_hour = hourly.groupby(hourly.index.hour).mean()

# Plotar consumo médio por hora do dia
plt.figure(figsize=(10,5))
mean_by_hour.plot(kind='bar')
plt.title("Consumo médio por hora do dia")
plt.xlabel("Hora do dia")
plt.ylabel("Consumo médio (kW)")
plt.show()

"""### **Análise do Gráfico:**

O gráfico mostra que os horários de maior consumo médio de energia ocorrem entre 7h–9h (primeiro pico matinal) e 19h–21h (pico principal noturno), enquanto os horários de menor consumo são na madrugada (2h–5h).

# **Exercício 22**

Autocorrelação do consumo
"""

import pandas as pd
from statsmodels.graphics.tsaplots import plot_acf

file_path = '/content/drive/MyDrive/ColabData/household_power_consumption.txt'

# Carregar o dataset
df = pd.read_csv(file_path, sep=";", na_values="?", low_memory=False)

# Converter a coluna 'Date' para datetime
df["Date"] = pd.to_datetime(df["Date"], format="%d/%m/%Y", errors='coerce')

# Criar a coluna 'Datetime' combinando 'Date' com 'Time'
df["Datetime"] = pd.to_datetime(
    df["Date"].dt.strftime("%Y-%m-%d") + " " + df["Time"],
    format="%Y-%m-%d %H:%M:%S",
    errors='coerce'
)

# Definir o índice como Datetime
df.set_index("Datetime", inplace=True)

# Remover valores nulos da série
series = df["Global_active_power"].dropna()

# Calcular autocorrelação para lags de 1h, 24h e 48h (dados por minuto)
lags = [60, 60*24, 60*48]

print("📊 Autocorrelação da série Global_active_power:")
for lag in lags:
    autocorr = series.autocorr(lag=lag)
    horas = lag // 60
    print(f"→ Lag de {horas}h ({lag} min): {autocorr:.4f}")

# Usar apenas os primeiros 5000 pontos (ou menos)
subset = series.iloc[:5000]

plt.figure(figsize=(10, 4))
autocorrelation_plot(subset)
plt.title("Autocorrelação (amostra reduzida)")
plt.xlabel("Lag (minutos)")
plt.ylabel("Autocorrelação")
plt.grid(True)
plt.show()

"""### **Análise**

A autocorrelação indica que há padrões diários no consumo de energia, com repetições em lags próximos de 1440 minutos (24h). Isso sugere ciclos típicos como picos pela manhã e à noite.

# **Exercicio 23**
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Selecionar as colunas
features = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']
X = df[features].dropna()

# Padronizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Mostrar variância explicada
print("Variância explicada por cada componente:")
print(pca.explained_variance_ratio_)

"""### **Análise**

O resultado mostra que:

- O primeiro componente explica cerca de 59,1% da variância, o que indica que ele captura a maior parte da estrutura dos dados — provavelmente uma combinação forte entre consumo ativo e intensidade elétrica.

- O segundo componente explica cerca de 22,7%, adicionando mais informação, mas com menos impacto.

Juntas, essas duas componentes explicam aproximadamente 81,8% da variância total, o que é excelente para uma redução de dimensionalidade. Isso significa que você manteve a maior parte da informação original com apenas dois eixos — ideal para visualização e clustering.

# **Exercício 24**
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Aplicar K-Means com 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_pca)

X_pca_sample = X_pca[:3000]  # ou até menos, tipo 1000
clusters_sample = clusters[:3000]

plt.scatter(X_pca_sample[:, 0], X_pca_sample[:, 1], c=clusters_sample, cmap='viridis')
plt.title('Clusters no PCA (amostra)')
plt.show()

"""### **Análise**

Os grupos se dividem de forma relativamente clara no gráfico. As cores dos clusters estão bem separadas espacialmente, com pouca sobreposição entre os pontos.

# **Exercício 25**
"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
import numpy as np

# Selecionar e limpar os dados
df_clean = df[['Voltage', 'Global_active_power']].dropna()
X = df_clean[['Voltage']]
y = df_clean['Global_active_power']

linear_model = LinearRegression()
linear_model.fit(X, y)
y_pred_linear = linear_model.predict(X)
rmse_linear = np.sqrt(mean_squared_error(y, y_pred_linear))

# Regressão Polinomial (grau 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

poly_model = LinearRegression()
poly_model.fit(X_poly, y)
y_pred_poly = poly_model.predict(X_poly)
rmse_poly = np.sqrt(mean_squared_error(y, y_pred_poly))

X_flat = X.values.flatten()
sorted_idx = X_flat.argsort()

plt.figure(figsize=(10, 6))
plt.scatter(X_flat[:1000], y[:1000], color='gray', alpha=0.3, label='Dados reais')
plt.plot(X_flat[sorted_idx][:1000], y_pred_linear[sorted_idx][:1000], color='blue', label=f'Linear (RMSE={rmse_linear:.2f})')
plt.plot(X_flat[sorted_idx][:1000], y_pred_poly[sorted_idx][:1000], color='red', label=f'Polinomial (RMSE={rmse_poly:.2f})')
plt.xlabel('Voltage')
plt.ylabel('Global_active_power')
plt.title('Regressão Linear vs Polinomial (grau 2)')
plt.legend()
plt.grid(True)
plt.show()

"""### **Análise**

A regressão linear apresentou maior erro (RMSE), indicando que não representa bem a relação entre Voltage e Global_active_power. Já a regressão polinomial de grau 2 conseguiu se ajustar melhor aos dados, acompanhando suas variações com mais precisão. Visualmente, a curva polinomial se aproxima mais dos pontos reais. O RMSE menor confirma que esse modelo é mais eficaz. Portanto, o ajuste polinomial é preferível neste caso.

# **Exercício 26**
"""

import pandas as pd

filepath = '/content/drive/MyDrive/ColabData/energydata_complete.csv'

# Carregando o dataset do Drive
df = pd.read_csv(filepath)

# Inspeção inicial
print(df.info())
print(df.describe())

"""# **Exercício 27**"""

import matplotlib.pyplot as plt

# Histograma
plt.figure(figsize=(10, 5))
plt.hist(df['Appliances'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribuição do Consumo de Eletrodomésticos (Wh)')
plt.xlabel('Consumo (Wh)')
plt.ylabel('Frequência')
plt.grid(True)
plt.show()

# Série temporal
df['date'] = pd.to_datetime(df['date'])
plt.figure(figsize=(15, 5))
plt.plot(df['date'], df['Appliances'], color='darkgreen')
plt.title('Consumo ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Consumo (Wh)')
plt.tight_layout()
plt.show()

"""### **Análise**

O histograma geralmente mostra que o consumo tende a se concentrar em valores mais baixos, com picos ocasionais — o que é típico em residências com uso eficiente de energia.

# **Exercício 28**
"""

# Selecionando variáveis ambientais
ambient_vars = ['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']

# Correlações com Appliances
correlations = df[ambient_vars + ['Appliances']].corr()['Appliances'].drop('Appliances')
print(correlations.sort_values(ascending=False))

"""### **Análise**

Você pode observar que algumas variáveis como T2 (temperatura da sala de estar) ou RH_1 (umidade da cozinha) tendem a ter correlação mais forte com o consumo. Isso faz sentido, já que ambientes mais usados influenciam diretamente o uso de eletrodomésticos.

# **Exercício 29**
"""

from sklearn.preprocessing import MinMaxScaler

# Selecionando variáveis numéricas
numeric_cols = df.select_dtypes(include='number').columns

# Aplicando Min-Max Scaling
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols)

# Visualizando os dados normalizados
print(df_scaled.head())

"""# **Exercício 30**"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Aplicando PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

# Criando DataFrame com os componentes
df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])

# Plotando os dados
plt.figure(figsize=(10, 6))
plt.scatter(df_pca['PC1'], df_pca['PC2'], alpha=0.3, color='teal')
plt.title('PCA - 2 Componentes Principais')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.grid(True)
plt.show()

"""### **Análise**

Se surgirem agrupamentos ou padrões visuais, isso pode indicar que certos tipos de consumo ou condições ambientais se repetem. Se quiser, podemos aplicar clustering (como K-Means) para investigar esses padrões.

# **Exercício 31**
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np

# Selecionando variáveis ambientais
X = df_scaled[['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']]
y = df_scaled['Appliances']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo
lr = LinearRegression()
lr.fit(X_train, y_train)

# Avaliação
y_pred_lr = lr.predict(X_test)
r2 = r2_score(y_test, y_pred_lr)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))

print(f"R²: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

"""# **Exercício 32**"""

from sklearn.ensemble import RandomForestRegressor

# Selecionando variáveis ambientais
X = df_scaled[['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',
               'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']]
y = df_scaled['Appliances']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando Random Forest com menos estimadores e paralelização
rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# Avaliação
y_pred_rf = rf.predict(X_test)
r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

print(f"R² - Random Forest: {r2_rf:.4f}")
print(f"RMSE - Random Forest: {rmse_rf:.4f}")

"""### **Comparação**

O Random Forest geralmente apresenta menor RMSE, indicando melhor desempenho em prever o consumo.

# **Exercício 33**
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Usando apenas Appliances e variáveis ambientais
X_cluster = df_scaled[['Appliances'] + list(X.columns)]

# Aplicando K-Means com 4 clusters
kmeans = KMeans(n_clusters=4, random_state=42)
df_scaled['Cluster'] = kmeans.fit_predict(X_cluster)

# Visualizando os clusters
plt.figure(figsize=(10, 6))
for cluster in range(4):
    subset = df_scaled[df_scaled['Cluster'] == cluster]
    plt.scatter(subset['T2'], subset['Appliances'], label=f'Cluster {cluster}', alpha=0.5)

plt.title('Perfis de Consumo por Cluster')
plt.xlabel('Temperatura Sala de Estar (T2)')
plt.ylabel('Consumo Normalizado (Appliances)')
plt.legend()
plt.grid(True)
plt.show()

"""### **Análise**

Os clusters podem revelar padrões como:

- Consumo alto em ambientes quentes

- Consumo baixo em ambientes úmidos

- Perfis intermediários com variações sazonais

# **Exercício 34**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Criando variável binária
median_consumo = df['Appliances'].median()
df_scaled['consumo_binario'] = (df['Appliances'] > median_consumo).astype(int)

# Features e target
X = df_scaled[['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',
               'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']]
y = df_scaled['consumo_binario']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando modelos
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)

rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)
rf_clf.fit(X_train, y_train)

"""# **Exercício 35**"""

from sklearn.metrics import confusion_matrix, classification_report

# Previsões
y_pred_logreg = logreg.predict(X_test)
y_pred_rf = rf_clf.predict(X_test)

# Avaliação - Logistic Regression
print("🔍 Logistic Regression")
print(confusion_matrix(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))

# Avaliação - Random Forest
print("🌲 Random Forest Classifier")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

"""### **Análise**

- A matriz de confusão mostra onde o modelo mais erra: se confunde mais os casos de alto consumo ou baixo consumo.

- O recall para cada classe indica a capacidade do modelo de identificar corretamente os casos positivos (alto consumo) ou negativos (baixo consumo).

- O F1-score equilibra precisão e recall — ótimo para avaliar modelos com classes desbalanceadas.
"""