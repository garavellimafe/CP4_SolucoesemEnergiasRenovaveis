# -*- coding: utf-8 -*-
"""CP4  - Parte 1, 2 e 3- 1CCPF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11huo-lVoMr3d4-1s_LdSTPdYpGdO4irM

# **CP4 - Solu√ß√µes em Energias Renovaveis - Parte 1, 2 e 3**

**Turma:** 1CCPF

**Integrantes:**
- MariaFernanda Garavelli Dantas RM: 562686
- Rog√©rio Deligi Ferreira Filho RM: 561942
- Guilherme Morais de Assis  RM: 564198

# **Inicio da CP4**

Primeiramente baixando o arquivo household_power_comsumption.txt
"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/ColabData/household_power_consumption.txt'

"""# **Exerc√≠cio 1**

Carregando o dataset e imprimindo as primeiras 10 linhas dele
"""

import pandas as pd

# Carregar o dataset (substitua o caminho pelo local onde voc√™ salvou o arquivo .txt ou .csv)
df = pd.read_csv(file_path, sep=";", na_values="?", low_memory=False)

# Exibir as 10 primeiras linhas
print(df.head(10))

"""# **Exerc√≠cio 2**

Explica√ß√£o da diferen√ßa entre Global_active_power e Global_reactive_power

***Diferen√ßa entre Global_active_power e Global_reactive_power***

  - *Global_active_power* (Pot√™ncia Ativa)
√â a pot√™ncia efetivamente consumida/convertida em trabalho √∫til (W ‚Üí kW).
Exemplo: ligar uma l√¢mpada, TV ou geladeira.
Essa energia √© a que de fato aparece na conta de luz.

  - *Global_reactive_power* (Pot√™ncia Reativa)
√â a pot√™ncia associada aos campos el√©tricos e magn√©ticos de dispositivos (motores, transformadores, etc.).
N√£o realiza trabalho √∫til, mas √© necess√°ria para manter os equipamentos funcionando.
√â medida em kVAR (kilovolt-ampere reativo).

# **Exerc√≠cio 3**

Verificando valores ausentes no dataset
"""

# Verificar valores ausentes
missing_values = df.isnull().sum()

# Quantidade total de valores ausentes
total_missing = df.isnull().sum().sum()

print("Valores ausentes por coluna:\n", missing_values)
print("\nTotal de valores ausentes:", total_missing)

"""# **Exerc√≠cio 4**

Convertendo Date para datetime e criar coluna do dia da semana
"""

# Converter a coluna 'Date' para datetime
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

# Criar nova coluna com o dia da semana
df['Weekday'] = df['Date'].dt.day_name()

# Visualizar exemplo
print(df[['Date', 'Weekday']].head(10))

"""# **Exerc√≠cio 5**

Filtrando registros de 2007 e calculando m√©dia di√°ria do consumo (Global_active_power)
"""

# Filtrar apenas 2007
df_2007 = df[df['Date'].dt.year == 2007]

# M√©dia di√°ria
media_diaria_2007 = df_2007.groupby(df_2007['Date'])['Global_active_power'].mean()
print(media_diaria_2007.head())

"""# **Exerc√≠cio 6**

Gerando gr√°fico de linha de um √∫nico dia
"""

import matplotlib.pyplot as plt

# Selecionar um dia (exemplo: 1¬∫ de mar√ßo de 2007)
um_dia = df_2007[df_2007['Date'] == '2007-03-01']

plt.figure(figsize=(12,5))
plt.plot(um_dia['Time'], um_dia['Global_active_power'])
plt.title("Varia√ß√£o do Consumo em 01/03/2007")
plt.xlabel("Hora do dia")
plt.ylabel("Pot√™ncia ativa (kW)")
plt.xticks(rotation=45)
plt.show()

"""# **Exerc√≠cio 7**

Gerando histograma do Voltage
"""

plt.hist(df['Voltage'].dropna(), bins=50, color='skyblue', edgecolor='black')
plt.title("Distribui√ß√£o da Tens√£o (Voltage)")
plt.xlabel("Voltage (V)")
plt.ylabel("Frequ√™ncia")
plt.show()

"""### **Algumas Observa√ß√µes**

- A distribui√ß√£o √© aproximadamente normal (sim√©trica).
- O valor m√©dio de tens√£o est√° em torno de 240 V.
- A maioria das medi√ß√µes ocorre entre 235 V e 245 V.
- Varia√ß√µes extremas (<230 V ou >250 V) s√£o raras.

# **Exerc√≠cio 8**

Calculando consumo m√©dio por m√™s em todo o per√≠odo
"""

df['YearMonth'] = df['Date'].dt.to_period('M')

consumo_mensal = df.groupby('YearMonth')['Global_active_power'].mean()
print(consumo_mensal)

"""# **Exerc√≠cio 9**

Identificando o dia com maior consumo de energia ativa
"""

consumo_diario = df.groupby('Date')['Global_active_power'].sum()

dia_max = consumo_diario.idxmax()
print("Dia com maior consumo:", dia_max, "‚Üí", consumo_diario.max())

"""# **Exerc√≠cio 10**

Comparando consumo m√©dio: dias de semana vs finais de semana
"""

df['Weekday'] = df['Date'].dt.dayofweek  # 0=segunda, 6=domingo
df['is_weekend'] = df['Weekday'] >= 5

media_weekday = df.groupby('is_weekend')['Global_active_power'].mean()
print(media_weekday)

"""# **Exerc√≠cio 11**

Calculando correla√ß√£o entre vari√°veis
"""

corr = df[['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']].corr()
print(corr)

"""# **Exerc√≠cio 12**

Criando nova vari√°vel Total_Sub_metering
"""

df['Total_Sub_metering'] = df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3']

"""# **Exerc√≠cio 13**

Verificando se algum m√™s teve Total_Sub_metering maior que m√©dia de Global_active_power
"""

comparacao = df.groupby('YearMonth')[['Total_Sub_metering', 'Global_active_power']].mean()

resultado = comparacao[comparacao['Total_Sub_metering'] > comparacao['Global_active_power']]
print(resultado)

"""# **Exerc√≠cio 14**

Gerando gr√°fio Voltage para o ano de 2008
"""

# Converta a coluna Time para timedelta
df['Time'] = pd.to_timedelta(df['Time'])

# Crie a coluna DateTime somando Date + Time
df['DateTime'] = df['Date'] + df['Time']

df_2008 = df[df['DateTime'].dt.year == 2008]

plt.figure(figsize=(12,5))
plt.plot(df_2008['DateTime'], df_2008['Voltage'], color='orange')
plt.title("S√©rie Temporal do Voltage - 2008")
plt.xlabel("Data")
plt.ylabel("Voltage (V)")
plt.show()

"""# **Exerc√≠cio 15**

Comparando consumo: ver√£o x inverno (hemisf√©rio norte)
"""

# Ver√£o
verao = df[df['DateTime'].dt.month.isin([6,7,8])]
# Inverno
inverno = df[df['DateTime'].dt.month.isin([12,1,2])]

print("Consumo m√©dio ver√£o:", verao['Global_active_power'].mean())
print("Consumo m√©dio inverno:", inverno['Global_active_power'].mean())

"""# **Exerc√≠cio 16**

Aplicando amostragem e verificando distribui√ß√£o da Global_active_power com a base completa
"""

sample = df.sample(frac=0.01, random_state=42)

plt.figure(figsize=(12,5))
df['Global_active_power'].hist(alpha=0.5, bins=50, label='Base Completa')
sample['Global_active_power'].hist(alpha=0.5, bins=50, label='Amostra 1%')
plt.legend()
plt.title("Compara√ß√£o da distribui√ß√£o de Global_active_power")
plt.show()

"""# **Exerc√≠cio 17**

Utilizando min-max scaling para padronizar as variaveis num√©ricas
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_norm = df[['Global_active_power','Global_reactive_power','Voltage','Global_intensity']].copy()
df_norm[df_norm.columns] = scaler.fit_transform(df_norm)

df_norm.head()

"""# **Exerc√≠cio 18**

Aplicando K-means para segmentar os 3 grupos distintos de consumo el√©trico
"""

# Certifique-se de que a coluna 'Date' est√° no formato datetime
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

# Agrupar por dia e calcular a m√©dia de 'Global_active_power'
df_dia = df.groupby('Date')['Global_active_power'].mean().reset_index()

# Renomear a coluna para refletir que √© uma m√©dia
df_dia.rename(columns={'Global_active_power': 'Mean_Global_active_power'}, inplace=True)

from sklearn.cluster import KMeans

df_dia['Mean_Global_active_power'] = df_dia['Mean_Global_active_power'].fillna(df_dia['Mean_Global_active_power'].mean())

kmeans = KMeans(n_clusters=3, random_state=42)
df_dia['Cluster'] = kmeans.fit_predict(df_dia[['Mean_Global_active_power']])

# Gr√°fico
plt.figure(figsize=(12,5))
plt.scatter(df_dia['Date'], df_dia['Mean_Global_active_power'], c=df_dia['Cluster'], cmap='viridis')
plt.title("Segmenta√ß√£o de Consumo Di√°rio (KMeans)")
plt.xlabel("Data")
plt.ylabel("M√©dia Global Active Power")
plt.show()

"""### **Interpreta√ß√£o do Gr√°fico:**

O gr√°fico mostra a segmenta√ß√£o do consumo di√°rio ao longo do tempo (2007‚Äì2011) utilizando o algoritmo K-Means.

 -	Cada ponto representa o consumo di√°rio em um determinado dia.

 -  As cores representam diferentes grupos (clusters) de consumo detectados pelo K-Means.

 -  Observa-se que existem tr√™s padr√µes principais:

 - Grupo amarelo ‚Üí consumo baixo, mais est√°vel.

 -  Grupo roxo ‚Üí consumo m√©dio, predominante na s√©rie.

 - Grupo verde ‚Üí consumo mais alto, com picos sazonais que aparecem em certos per√≠odos do ano.

Em resumo: o consumo apresenta ciclos sazonais, com momentos de maior intensidade (clusters verdes), mas a maior parte dos dias concentra-se em n√≠veis m√©dios (roxo) e baixos (amarelo).

# **Exerc√≠cio 19**

Decompondo de serie temporal (sazonalidade, tend√™ncias e res√≠duo) para Global_active_power em um periodo de 6 meses
"""

from statsmodels.tsa.seasonal import seasonal_decompose

df_6m = df.set_index('DateTime').resample('D')['Global_active_power'].mean().dropna().head(180)

result = seasonal_decompose(df_6m, model='additive', period=30) # per√≠odo mensal
result.plot()
plt.show()

"""# **Exerc√≠cio 20**

Treinando modelo de regress√£o linear simples para prever Global_active_power a partir de Global_intensity
"""

#Preenchendo valores faltantes
df['Global_intensity'] = df['Global_intensity'].fillna(df['Global_intensity'].mean())
df['Global_active_power'] = df['Global_active_power'].fillna(df['Global_active_power'].mean())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

X = df[['Global_intensity']]
y = df['Global_active_power']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Erro quadr√°tico m√©dio (MSE):", mean_squared_error(y_test, y_pred))
print("Coeficiente:", model.coef_)
print("Intercepto:", model.intercept_)

"""### **Avaliando o erro:**

O modelo de regress√£o linear mostra uma rela√ß√£o direta entre intensidade el√©trica e consumo de energia, com cada aumento de 1 Ampere resultando em cerca de 0,238 kW adicionais. O MSE baixo (0,00247) indica boa precis√£o nas previs√µes. O intercepto pr√≥ximo de zero faz sentido fisicamente, pois sem corrente n√£o h√° consumo. No geral, o modelo simples captura bem o comportamento observado.

# **CP4 - Parte 2**

# **Exercic√≠o 21**

Series temporais por hora
"""

# -----------------------------
# Carregar e preparar os dados
# -----------------------------
df = pd.read_csv("household_power_consumption.txt", sep=";",
                parse_dates={"Datetime": ["Date", "Time"]},
                infer_datetime_format=True,
                na_values="?",
                low_memory=False)

# Converter para datetime e definir como √≠ndice
df['Datetime'] = pd.to_datetime(df['Datetime'])
df.set_index('Datetime', inplace=True)
# Converter Global_active_power para num√©rico
df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors="coerce")

# Reamostrar em intervalos de 1h (m√©dia)
hourly = df['Global_active_power'].resample('1H').mean()
# M√©dia por hora do dia (0h at√© 23h)
mean_by_hour = hourly.groupby(hourly.index.hour).mean()

# Plotar consumo m√©dio por hora do dia
plt.figure(figsize=(10,5))
mean_by_hour.plot(kind='bar')
plt.title("Consumo m√©dio por hora do dia")
plt.xlabel("Hora do dia")
plt.ylabel("Consumo m√©dio (kW)")
plt.show()

"""### **An√°lise do Gr√°fico:**

O gr√°fico mostra que os hor√°rios de maior consumo m√©dio de energia ocorrem entre 7h‚Äì9h (primeiro pico matinal) e 19h‚Äì21h (pico principal noturno), enquanto os hor√°rios de menor consumo s√£o na madrugada (2h‚Äì5h).

# **Exerc√≠cio 22**

Autocorrela√ß√£o do consumo
"""

import pandas as pd
from statsmodels.graphics.tsaplots import plot_acf

file_path = '/content/drive/MyDrive/ColabData/household_power_consumption.txt'

# Carregar o dataset
df = pd.read_csv(file_path, sep=";", na_values="?", low_memory=False)

# Converter a coluna 'Date' para datetime
df["Date"] = pd.to_datetime(df["Date"], format="%d/%m/%Y", errors='coerce')

# Criar a coluna 'Datetime' combinando 'Date' com 'Time'
df["Datetime"] = pd.to_datetime(
    df["Date"].dt.strftime("%Y-%m-%d") + " " + df["Time"],
    format="%Y-%m-%d %H:%M:%S",
    errors='coerce'
)

# Definir o √≠ndice como Datetime
df.set_index("Datetime", inplace=True)

# Remover valores nulos da s√©rie
series = df["Global_active_power"].dropna()

# Calcular autocorrela√ß√£o para lags de 1h, 24h e 48h (dados por minuto)
lags = [60, 60*24, 60*48]

print("üìä Autocorrela√ß√£o da s√©rie Global_active_power:")
for lag in lags:
    autocorr = series.autocorr(lag=lag)
    horas = lag // 60
    print(f"‚Üí Lag de {horas}h ({lag} min): {autocorr:.4f}")

# Usar apenas os primeiros 5000 pontos (ou menos)
subset = series.iloc[:5000]

plt.figure(figsize=(10, 4))
autocorrelation_plot(subset)
plt.title("Autocorrela√ß√£o (amostra reduzida)")
plt.xlabel("Lag (minutos)")
plt.ylabel("Autocorrela√ß√£o")
plt.grid(True)
plt.show()

"""### **An√°lise**

A autocorrela√ß√£o indica que h√° padr√µes di√°rios no consumo de energia, com repeti√ß√µes em lags pr√≥ximos de 1440 minutos (24h). Isso sugere ciclos t√≠picos como picos pela manh√£ e √† noite.

# **Exercicio 23**
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Selecionar as colunas
features = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']
X = df[features].dropna()

# Padronizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Mostrar vari√¢ncia explicada
print("Vari√¢ncia explicada por cada componente:")
print(pca.explained_variance_ratio_)

"""### **An√°lise**

O resultado mostra que:

- O primeiro componente explica cerca de 59,1% da vari√¢ncia, o que indica que ele captura a maior parte da estrutura dos dados ‚Äî provavelmente uma combina√ß√£o forte entre consumo ativo e intensidade el√©trica.

- O segundo componente explica cerca de 22,7%, adicionando mais informa√ß√£o, mas com menos impacto.

Juntas, essas duas componentes explicam aproximadamente 81,8% da vari√¢ncia total, o que √© excelente para uma redu√ß√£o de dimensionalidade. Isso significa que voc√™ manteve a maior parte da informa√ß√£o original com apenas dois eixos ‚Äî ideal para visualiza√ß√£o e clustering.

# **Exerc√≠cio 24**
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Aplicar K-Means com 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_pca)

X_pca_sample = X_pca[:3000]  # ou at√© menos, tipo 1000
clusters_sample = clusters[:3000]

plt.scatter(X_pca_sample[:, 0], X_pca_sample[:, 1], c=clusters_sample, cmap='viridis')
plt.title('Clusters no PCA (amostra)')
plt.show()

"""### **An√°lise**

Os grupos se dividem de forma relativamente clara no gr√°fico. As cores dos clusters est√£o bem separadas espacialmente, com pouca sobreposi√ß√£o entre os pontos.

# **Exerc√≠cio 25**
"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
import numpy as np

# Selecionar e limpar os dados
df_clean = df[['Voltage', 'Global_active_power']].dropna()
X = df_clean[['Voltage']]
y = df_clean['Global_active_power']

linear_model = LinearRegression()
linear_model.fit(X, y)
y_pred_linear = linear_model.predict(X)
rmse_linear = np.sqrt(mean_squared_error(y, y_pred_linear))

# Regress√£o Polinomial (grau 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

poly_model = LinearRegression()
poly_model.fit(X_poly, y)
y_pred_poly = poly_model.predict(X_poly)
rmse_poly = np.sqrt(mean_squared_error(y, y_pred_poly))

X_flat = X.values.flatten()
sorted_idx = X_flat.argsort()

plt.figure(figsize=(10, 6))
plt.scatter(X_flat[:1000], y[:1000], color='gray', alpha=0.3, label='Dados reais')
plt.plot(X_flat[sorted_idx][:1000], y_pred_linear[sorted_idx][:1000], color='blue', label=f'Linear (RMSE={rmse_linear:.2f})')
plt.plot(X_flat[sorted_idx][:1000], y_pred_poly[sorted_idx][:1000], color='red', label=f'Polinomial (RMSE={rmse_poly:.2f})')
plt.xlabel('Voltage')
plt.ylabel('Global_active_power')
plt.title('Regress√£o Linear vs Polinomial (grau 2)')
plt.legend()
plt.grid(True)
plt.show()

"""### **An√°lise**

A regress√£o linear apresentou maior erro (RMSE), indicando que n√£o representa bem a rela√ß√£o entre Voltage e Global_active_power. J√° a regress√£o polinomial de grau 2 conseguiu se ajustar melhor aos dados, acompanhando suas varia√ß√µes com mais precis√£o. Visualmente, a curva polinomial se aproxima mais dos pontos reais. O RMSE menor confirma que esse modelo √© mais eficaz. Portanto, o ajuste polinomial √© prefer√≠vel neste caso.

# **Exerc√≠cio 26**
"""

import pandas as pd

filepath = '/content/drive/MyDrive/ColabData/energydata_complete.csv'

# Carregando o dataset do Drive
df = pd.read_csv(filepath)

# Inspe√ß√£o inicial
print(df.info())
print(df.describe())

"""# **Exerc√≠cio 27**"""

import matplotlib.pyplot as plt

# Histograma
plt.figure(figsize=(10, 5))
plt.hist(df['Appliances'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribui√ß√£o do Consumo de Eletrodom√©sticos (Wh)')
plt.xlabel('Consumo (Wh)')
plt.ylabel('Frequ√™ncia')
plt.grid(True)
plt.show()

# S√©rie temporal
df['date'] = pd.to_datetime(df['date'])
plt.figure(figsize=(15, 5))
plt.plot(df['date'], df['Appliances'], color='darkgreen')
plt.title('Consumo ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Consumo (Wh)')
plt.tight_layout()
plt.show()

"""### **An√°lise**

O histograma geralmente mostra que o consumo tende a se concentrar em valores mais baixos, com picos ocasionais ‚Äî o que √© t√≠pico em resid√™ncias com uso eficiente de energia.

# **Exerc√≠cio 28**
"""

# Selecionando vari√°veis ambientais
ambient_vars = ['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']

# Correla√ß√µes com Appliances
correlations = df[ambient_vars + ['Appliances']].corr()['Appliances'].drop('Appliances')
print(correlations.sort_values(ascending=False))

"""### **An√°lise**

Voc√™ pode observar que algumas vari√°veis como T2 (temperatura da sala de estar) ou RH_1 (umidade da cozinha) tendem a ter correla√ß√£o mais forte com o consumo. Isso faz sentido, j√° que ambientes mais usados influenciam diretamente o uso de eletrodom√©sticos.

# **Exerc√≠cio 29**
"""

from sklearn.preprocessing import MinMaxScaler

# Selecionando vari√°veis num√©ricas
numeric_cols = df.select_dtypes(include='number').columns

# Aplicando Min-Max Scaling
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols)

# Visualizando os dados normalizados
print(df_scaled.head())

"""# **Exerc√≠cio 30**"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Aplicando PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

# Criando DataFrame com os componentes
df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])

# Plotando os dados
plt.figure(figsize=(10, 6))
plt.scatter(df_pca['PC1'], df_pca['PC2'], alpha=0.3, color='teal')
plt.title('PCA - 2 Componentes Principais')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.grid(True)
plt.show()

"""### **An√°lise**

Se surgirem agrupamentos ou padr√µes visuais, isso pode indicar que certos tipos de consumo ou condi√ß√µes ambientais se repetem. Se quiser, podemos aplicar clustering (como K-Means) para investigar esses padr√µes.

# **Exerc√≠cio 31**
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np

# Selecionando vari√°veis ambientais
X = df_scaled[['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']]
y = df_scaled['Appliances']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo
lr = LinearRegression()
lr.fit(X_train, y_train)

# Avalia√ß√£o
y_pred_lr = lr.predict(X_test)
r2 = r2_score(y_test, y_pred_lr)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))

print(f"R¬≤: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

"""# **Exerc√≠cio 32**"""

from sklearn.ensemble import RandomForestRegressor

# Selecionando vari√°veis ambientais
X = df_scaled[['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',
               'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']]
y = df_scaled['Appliances']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando Random Forest com menos estimadores e paraleliza√ß√£o
rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# Avalia√ß√£o
y_pred_rf = rf.predict(X_test)
r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

print(f"R¬≤ - Random Forest: {r2_rf:.4f}")
print(f"RMSE - Random Forest: {rmse_rf:.4f}")

"""### **Compara√ß√£o**

O Random Forest geralmente apresenta menor RMSE, indicando melhor desempenho em prever o consumo.

# **Exerc√≠cio 33**
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Usando apenas Appliances e vari√°veis ambientais
X_cluster = df_scaled[['Appliances'] + list(X.columns)]

# Aplicando K-Means com 4 clusters
kmeans = KMeans(n_clusters=4, random_state=42)
df_scaled['Cluster'] = kmeans.fit_predict(X_cluster)

# Visualizando os clusters
plt.figure(figsize=(10, 6))
for cluster in range(4):
    subset = df_scaled[df_scaled['Cluster'] == cluster]
    plt.scatter(subset['T2'], subset['Appliances'], label=f'Cluster {cluster}', alpha=0.5)

plt.title('Perfis de Consumo por Cluster')
plt.xlabel('Temperatura Sala de Estar (T2)')
plt.ylabel('Consumo Normalizado (Appliances)')
plt.legend()
plt.grid(True)
plt.show()

"""### **An√°lise**

Os clusters podem revelar padr√µes como:

- Consumo alto em ambientes quentes

- Consumo baixo em ambientes √∫midos

- Perfis intermedi√°rios com varia√ß√µes sazonais

# **Exerc√≠cio 34**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Criando vari√°vel bin√°ria
median_consumo = df['Appliances'].median()
df_scaled['consumo_binario'] = (df['Appliances'] > median_consumo).astype(int)

# Features e target
X = df_scaled[['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',
               'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9']]
y = df_scaled['consumo_binario']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando modelos
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)

rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)
rf_clf.fit(X_train, y_train)

"""# **Exerc√≠cio 35**"""

from sklearn.metrics import confusion_matrix, classification_report

# Previs√µes
y_pred_logreg = logreg.predict(X_test)
y_pred_rf = rf_clf.predict(X_test)

# Avalia√ß√£o - Logistic Regression
print("üîç Logistic Regression")
print(confusion_matrix(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))

# Avalia√ß√£o - Random Forest
print("üå≤ Random Forest Classifier")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

"""### **An√°lise**

- A matriz de confus√£o mostra onde o modelo mais erra: se confunde mais os casos de alto consumo ou baixo consumo.

- O recall para cada classe indica a capacidade do modelo de identificar corretamente os casos positivos (alto consumo) ou negativos (baixo consumo).

- O F1-score equilibra precis√£o e recall ‚Äî √≥timo para avaliar modelos com classes desbalanceadas.
"""